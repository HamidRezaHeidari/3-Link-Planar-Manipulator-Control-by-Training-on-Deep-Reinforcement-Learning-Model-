"""
Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xaefXqzC3AZbPjd05TYLhMKDakLcaXyy

# AIinRobotics - HW2 - Hamid Reza Heidari

## Following Assumptions are made for the model:

### 1. Theta 1 is angle between arm one and horizontal
### 2. Theta 2 is angle between arm one and arm two    
### 3. Theta 3 is angle between arm two and arm three
### 4. The Manipulator is situated at origin

"""

# if face with error then uncomment these:

# !pip install shimmy
# !pip install stable-baselines3[extra]

# Import Libraries

from math import sin, cos, sqrt
import numpy as np
import matplotlib.pyplot as plt
import gym
from gym import spaces
from stable_baselines3 import PPO

# Define Forward Kinematic

def forward_kinematic(T, L):

    L1, L2, L3 = L

    T1, T2, T3 = T

    # Link coordinates of Arm 1
    x1 = L1*cos(T1)
    y1 = L1*sin(T1)

    # Link Coordinates of Arm 2
    x2 = x1 + L2*cos(T1 + T2)
    y2 = y1 + L2*sin(T1 + T2)

    # Link Coordinates of Arm 3
    x3 = x2 + L3*cos(T1 + T2 + T3)
    y3 = y2 + L3*sin(T1 + T2 + T3)

    return (0, 0), (x1, y1), (x2, y2), (x3, y3)

# Define Environment

class ThreeLinkManipulatorEnv(gym.Env):

    def __init__(self):

        super(ThreeLinkManipulatorEnv, self).__init__()

        # Length of the links
        self.link_lengths = [1.0, 1.0, 1.0]

        # Define initial and target point

        self.initil_point_state = [0, 0, 0]

        self.target_point = [1.6, 2]

        # Define action and observation space
        self.action_space = spaces.Box(low=-0.01, high=0.01, shape=(3,), dtype=np.float32)

        self.observation_space = spaces.Box(low=-np.pi, high=np.pi, shape=(3,), dtype=np.float32)

        # Initialize state (angles of the 3 links)
        self.state = self.initil_point_state



    def reset(self):
        self.state = self.initil_point_state

        return self.state

    def step(self, action):
        self.state = np.clip(self.state + action, -np.pi, np.pi)

        fk = forward_kinematic(self.state, self.link_lengths)
        end_effector_pos = fk[3]
        distance_to_target = np.linalg.norm(np.array(end_effector_pos) -  np.array(self.target_point))

        # Reward calculation and set hyperparameters

        self.alpha=0.4
        self.beta=0.6

        reward = 1/(distance_to_target + self.alpha*(np.sum(np.abs(self.state))) + self.beta*np.sum(np.square(action)) )

        done = distance_to_target < 0.0005

        return self.state, reward, done, {}

    # Render to Visualize the manipulator

    def render(self, mode='rgb_array'):

        fk = forward_kinematic(self.state, self.link_lengths)
        x0, y0 = fk[0]
        x1, y1 = fk[1]
        x2, y2 = fk[2]
        x3, y3 = fk[3]

        fig = plt.figure()
        plt.plot([x0, x1], [y0, y1], 'bo-', lw=3)
        plt.plot([x1, x2], [y1, y2], 'go-', lw=3)
        plt.plot([x2, x3], [y2, y3], 'mo-', lw=3)
        plt.plot(self.target_point[0], self.target_point[1], 'rx', markersize=8)
        plt.plot([],[], color="w")
        plt.plot([],[], color="w")
        plt.xlim(-5, 5)
        plt.ylim(-5, 5)
        plt.legend(["link1", "link2", "link3", "final point", f"alpha: {self.alpha}", f"beta : {self.beta}"], loc='lower left')
        plt.grid()
        plt.show()

# Create Env
env = ThreeLinkManipulatorEnv()

# Create the model
model = PPO("MlpPolicy", env, verbose=1)

# Train the model
for _ in range(15):

  model.learn(total_timesteps=10000, progress_bar=True)

# Test Model

obs = env.reset()

for i in range(250):

    action, _states = model.predict(obs, deterministic=True)
    obs, rewards, dones, info = env.step(action)
    env.render()

    if dones:
        break


env.close()

